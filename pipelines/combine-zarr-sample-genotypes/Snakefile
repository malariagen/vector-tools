import pandas as pd
import os
import pyfaidx

fa = pyfaidx.Fasta(config["fasta_path"])
contigs = list(fa.keys())

fofn_path = config["fofn"]
sampleset = os.path.basename(os.path.dirname(fofn_path))
fofn = pd.read_csv(config["fofn"], sep="\t")
samples = fofn["sample"].unique()

if "contigs" not in config:
  config["contigs"] = list(fa.keys())


rule all_merge:
  input:
    expand(
      os.path.join(config["samplesets_output_dir"], sampleset, "callset.zarr/{chrom}/calldata/{field}"),
      field=config["fields"],
      chrom=config["contigs"]
    ),
    os.path.join(config["samplesets_output_dir"], sampleset, "merge_genotypes_config.yml")


# NH: This is an input to the contamination pipeline. We didn't use this for phase 3 because we had the phase 2 dataset available, which was curated
rule all_freqs:
  input:
    expand(
      os.path.join(config["samplesets_output_dir"], sampleset, "allele_frequencies.zarr/{chrom}/AF"),
      chrom=config["contigs"]
    ),
    os.path.join(sampleset, "merge_genotypes_config.yml")

rule merge_zarr:
  input:
    manifest=config["samplesets_output_dir"] + "/{sampleset}/manifest"
  output:
    zarr=directory(config["samplesets_output_dir"] + "/{sampleset}/callset.zarr/{chrom}/calldata/{field}"),
    done=touch(config["samplesets_output_dir"] + "/{sampleset}/callset.zarr/{chrom}/calldata/{field}.complete")
  params:
    samples=lambda y: os.path.join(config["samplesets_output_dir"], y.sampleset, "manifest"),
    input_pattern=lambda y: os.path.join(config["samplesets_output_dir"], config["data_dir"], "{sample}" + config["gt_suffix"]),
    output=lambda y: os.path.join(config["samplesets_output_dir"], y.sampleset, "callset.zarr"),
    seqid=lambda y: y.chrom,
    field=lambda y: y.field,
    cname="zstd",
    clevel=1,
    chunk_width=config["chunk_width"],
    shuffle=0,
    num_workers=4,
    req="h_vmem=12G",
    pe="-pe simple_pe 4"
  script:
    "../../scripts/combine_zarr.py"


rule compute_allele_freqs:
  output:
    zarr=directory(config["samplesets_output_dir"] + "/{sampleset}/allele_frequencies.zarr/{chrom}/AF"),
    done=touch(config["samplesets_output_dir"] + "/{sampleset}/allele_frequencies.zarr/{chrom}/AF.complete"),
  input:
    zarr=config["samplesets_output_dir"] + "/{sampleset}/callset.zarr/{chrom}/calldata/GT",
    sites=config["sites_path"]
  params:
    outz=lambda y: os.path.join(config["samplesets_output_dir"], y.sampleset, "allele_frequencies.zarr"),
    inz=lambda y: os.path.join(config["samplesets_output_dir"], y.sampleset, "callset.zarr"),
    req="h_vmem=10G",
    pe=""
  run:
    import zarr
    import allel
    import numpy as np
    from pathlib import Path
    
    callset_fn = params.inz
    output_fn = params.outz
    chrom = wildcards.chrom
    
    zh = zarr.open_group(output_fn, "a")
    merged_callset = zarr.open_group(callset_fn, "r")

    if Path(input.sites).is_file():
      store = zarr.ZipStore(input.sites, mode='r')
      zsites = zarr.Group(store)

    else:
      zsites = zarr.open_group(input.sites, "r")
    
    gt = allel.GenotypeDaskArray(merged_callset[chrom]["calldata/GT"])
    positions = zsites[chrom]["variants/POS"][:]

    if not gt.is_called().any().compute():
      ac = np.zeros((positions.shape[0], 4), dtype="int")
      af = np.zeros((positions.shape[0], 4))
    else:
      ac = gt.count_alleles(max_allele=3).compute()
      af = ac.to_frequencies()    

    zch = zh.require_group(chrom)
    
    zch.create_dataset("POS", data=positions)
    zch.create_dataset("AF", data=af)
    zch.create_dataset("AC", data=ac)


rule dump_manifest:
  output:
    txt=config["samplesets_output_dir"] + "/{sampleset}/manifest"
  params:
    req="h_vmem=4G",
  run:
    with open(output.txt, mode="w") as wr:
      print("sample_name", file=wr)
      print("\n".join(samples), file=wr)


rule write_config:
  output:
    cfg=config["samplesets_output_dir"] + "/{sampleset}/merge_genotypes_config.yml"
  params:
    req="h_vmem=2G"
  run:
    import yaml
    with open(output.cfg, "w") as fh:
      yaml.dump(config, fh)

