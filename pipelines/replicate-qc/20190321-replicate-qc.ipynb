{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate QC\n",
    "\n",
    "This notebook takes a merged sampleset, ie all GT arrays from a set that have been merged by the `combine-zarr-callset` pipeline, and computes pairwise distances between them.\n",
    "\n",
    "Each contig is handled separately, and the dimensions of the resulting outputs are: contigs x npairs.\n",
    "\n",
    "NB: We restrict to bialleleic positions in phase 2. \n",
    "\n",
    "## Use\n",
    "\n",
    "0. Log onto `datalab.malariagen.net` and clone the repo with submodules.\n",
    "\n",
    "1. Copy this notebook and the replicate-qc-analysis one to the tracking directory of the sampleset you wish to perform replicate QC on.\n",
    "\n",
    "2. Ensure the `merged.zarr` is in the expected location on the datalab server.\n",
    "\n",
    "3. Run this notebook to compute distances\n",
    "\n",
    "4. Run the other notebook to discard samples that fail QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import allel\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleset = Path(\".\").absolute().name\n",
    "sampleset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = 'ag1000g-release/observatory/{sampleset}/callset.zarr'.format(sampleset=sampleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_fn = \"/gcs/observatory/{sampleset}/manifest\".format(sampleset=sampleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask-distance\n",
    "import dask_distance as dadist\n",
    "import scipy.spatial.distance as dist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to reshape for map_blocks\n",
    "def trans_d(block, metric=\"euclidean\"):\n",
    "    return dist.pdist(block, metric=metric).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning missing count\n",
    "def count_nmissing(X1, X2):\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    \n",
    "    # compress by non missing\n",
    "    ok = (X1 >= 0) & (X2 >= 0)\n",
    "    \n",
    "    # compute on array\n",
    "    return np.sum(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cityblock distance after pruning missings\n",
    "def cib_dist_nmissing(X1, X2):\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    \n",
    "    # compress by non missing\n",
    "    ok = (X1 >= 0) & (X2 >= 0)\n",
    "    \n",
    "    # compute on array\n",
    "    return dist.cityblock(\n",
    "        np.compress(ok, X1),\n",
    "        np.compress(ok, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS configuration\n",
    "import gcsfs\n",
    "\n",
    "gcs_bucket_fs = gcsfs.GCSFileSystem(\n",
    "    project='malariagen-jupyterhub', token='anon', access='read_only')\n",
    "\n",
    "store = gcsfs.mapping.GCSMap(\n",
    "    storage_path, gcs=gcs_bucket_fs, check=False, create=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calldata = zarr.Group(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(manifest_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume this is ok for now. Normally use the manifest\n",
    "samples = df[\"sample_name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kubernetes cluster setup\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=40)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask client setup\n",
    "from dask.distributed import Client, progress\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time taken scales exponentially with size of sampleset\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_callset = zarr.open_group(\"/gcs/phase2/AR1/variation/main/zarr2/ag1000g.phase2.ar1\")\n",
    "called_sites = zarr.open_group(\"/gcs/observatory/ag.allsites.nonN.zarr.zip\", mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find biallelic sites\n",
    "def find_phase2_bialleleic_sites(chrom):\n",
    "\n",
    "    g = allel.GenotypeDaskArray(phase2_callset[chrom][\"calldata\"][\"genotype\"])\n",
    "    \n",
    "    # TO DO PASS ONLY\n",
    "    \n",
    "    biallelic = (g.max(axis=[1,2]) <= 1).compute()\n",
    "                 \n",
    "    d = {}\n",
    "    for x in \"POS\", \"REF\", \"ALT\":\n",
    "        v = phase2_callset[chrom][\"variants\"][x]\n",
    "        dav = da.from_zarr(v, chunksize=v.chunks)\n",
    "        d[x] = da.compress(biallelic, dav, axis=0)\n",
    "        \n",
    "    return d[\"POS\"], d[\"ALT\"], d[\"REF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"pwd_contigs\": [\"3L\", \"3R\", \"2L\", \"2R\", \"X\"]}\n",
    "contigs = config[\"pwd_contigs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(combinations(range(len(samples)), 2))\n",
    "npairs = len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.zeros((len(contigs), npairs))\n",
    "denom = np.zeros((len(contigs), npairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_list = []\n",
    "for cix, contig in enumerate(contigs):\n",
    "\n",
    "    sites_pos = allel.SortedIndex(called_sites[contig][\"variants/POS\"])\n",
    "    bial_pos, bial_alt, bial_ref = find_phase2_bialleleic_sites(contig)\n",
    "    loc = sites_pos.locate_keys(bial_pos)\n",
    "\n",
    "    alleles=da.hstack((bial_ref.reshape((-1, 1)), bial_alt))\n",
    "\n",
    "    # reduce to biallelic sites all samples still\n",
    "    print(contig, \"compressing\")\n",
    "    gt_a = allel.GenotypeDaskArray(calldata[contig][\"calldata/GT\"]).compress(loc)\n",
    "\n",
    "    print(contig, \"remapping\")\n",
    "    mapping = allel.create_allele_mapping(\n",
    "        ref=np.compress(loc, called_sites[contig][\"variants/REF\"]),\n",
    "        alt=np.compress(loc, called_sites[contig][\"variants/ALT\"]),\n",
    "        alleles=alleles)\n",
    "\n",
    "    count_alts = gt_a.map_alleles(mapping).to_n_alt(fill=-1)\n",
    "    \n",
    "    alt_list.append(count_alts)\n",
    "\n",
    "    # transpose and rechunk for scipy dist object\n",
    "    ca = count_alts.T\n",
    "    ratio = ca.shape[0] / ca.chunksize[0]\n",
    "    newchunks = (ca.shape[0], int(ca.chunksize[1] / ratio))\n",
    "    ca = ca.rechunk(chunks=newchunks)\n",
    "    nchunks = len(ca.chunks[1])\n",
    "\n",
    "    D = ca.map_blocks(\n",
    "        trans_d, \n",
    "        metric=cib_dist_nmissing,\n",
    "        chunks=((1,), tuple(np.repeat(1, nchunks))), \n",
    "        dtype=float, \n",
    "        drop_axis=(0, ), \n",
    "        new_axis=(0, ))\n",
    "    \n",
    "    X = ca.map_blocks(\n",
    "        trans_d, \n",
    "        metric=count_nmissing,\n",
    "        chunks=((1,), tuple(np.repeat(1, nchunks))), \n",
    "        dtype=float, \n",
    "        drop_axis=(0, ), \n",
    "        new_axis=(0, ))\n",
    "    \n",
    "    h[cix] = D.compute().sum(axis=1)\n",
    "    denom[cix] = X.compute().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    \"replicate-qc-{sset}\".format(sset=sampleset), \n",
    "    cityblock=h, \n",
    "    nsites=denom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
