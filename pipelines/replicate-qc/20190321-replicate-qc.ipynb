{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate QC\n",
    "\n",
    "This notebook takes a merged sampleset, ie all GT arrays from a set that have been merged by the `combine-zarr-callset` pipeline, and computes pairwise distances between them.\n",
    "\n",
    "Each contig is handled separately, and the dimensions of the resulting outputs are: contigs x npairs.\n",
    "\n",
    "Three arrays are written, one with euclidean distance, one with cityblock distance and one with the number of comparable (ie called) sites.\n",
    "\n",
    "NB: We restrict to bialleleic positions in phase 2. \n",
    "\n",
    "NB: Efficiency could be improved markedly by chunking the genotypes with (X, 1, 2), to make it more efficient at reading in the relevant data. \n",
    "(I'm currently recreating on the cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import allel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask-distance in /opt/conda/lib/python3.6/site-packages (0.2.0)\n",
      "Requirement already satisfied: dask in /opt/conda/lib/python3.6/site-packages (from dask-distance) (1.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from dask-distance) (1.15.4)\n",
      "\u001b[33mYou are using pip version 19.0, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dask-distance\n",
    "import dask_distance as dadist\n",
    "import scipy.spatial.distance as dist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleset = \"AG1000G-UG\"\n",
    "\n",
    "# I wonder if used memory will scale better with chunk width = 1??\n",
    "chunksize = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = 'ag1000g-release/observatory/callset.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to reshape for map_blocks\n",
    "def trans_d(block, metric=\"euclidean\"):\n",
    "    return dist.pdist(block, metric=metric).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning missing count\n",
    "def count_nmissing(X1, X2):\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    \n",
    "    # compress by non missing\n",
    "    ok = (X1 >= 0) & (X2 >= 0)\n",
    "    \n",
    "    # compute on array\n",
    "    return np.sum(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cityblock distance after pruning missings\n",
    "def cib_dist_nmissing(X1, X2):\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    \n",
    "    # compress by non missing\n",
    "    ok = (X1 >= 0) & (X2 >= 0)\n",
    "    \n",
    "    # compute on array\n",
    "    return dist.cityblock(\n",
    "        np.compress(ok, X1),\n",
    "        np.compress(ok, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS configuration\n",
    "import gcsfs\n",
    "\n",
    "gcs_bucket_fs = gcsfs.GCSFileSystem(\n",
    "    project='malariagen-jupyterhub', token='anon', access='read_only')\n",
    "\n",
    "store = gcsfs.mapping.GCSMap(\n",
    "    storage_path, gcs=gcs_bucket_fs, check=False, create=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calldata = zarr.Group(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/gcs/observatory/manifest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume this is ok for now. Normally use the manifest\n",
    "samples = df[\"sample_name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c160a79875b64ae5a32e02c81c7c9b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = KubeCluster(n_workers=40)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.8.78.8:34327\n",
       "  <li><b>Dashboard: </b><a href='/user/nicholasharding/proxy/8787/status' target='_blank'>/user/nicholasharding/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.8.78.8:34327' processes=0 cores=0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_callset = zarr.open_group(\"/gcs/phase2/AR1/variation/main/zarr2/ag1000g.phase2.ar1\")\n",
    "called_sites = zarr.open_group(\"/gcs/observatory/ag.allsites.nonN.zarr.zip\", mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find biallelic sites\n",
    "def find_phase2_bialleleic_sites(chrom):\n",
    "\n",
    "    g = allel.GenotypeDaskArray(phase2_callset[chrom][\"calldata\"][\"genotype\"])\n",
    "    \n",
    "    # TO DO PASS ONLY\n",
    "    \n",
    "    biallelic = (g.max(axis=[1,2]) <= 1).compute()\n",
    "                 \n",
    "    d = {}\n",
    "    for x in \"POS\", \"REF\", \"ALT\":\n",
    "        v = phase2_callset[chrom][\"variants\"][x]\n",
    "        dav = da.from_zarr(v, chunksize=v.chunks)\n",
    "        d[x] = da.compress(biallelic, dav, axis=0)\n",
    "        \n",
    "    return d[\"POS\"], d[\"ALT\"], d[\"REF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"pwd_contigs\": [\"3L\", \"3R\", \"2L\", \"2R\", \"X\"]}\n",
    "contigs = config[\"pwd_contigs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(combinations(range(len(samples)), 2))\n",
    "npairs = len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.zeros((len(contigs), npairs))\n",
    "denom = np.zeros((len(contigs), npairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = count_alts.T\n",
    "ratio = ca.shape[0] / ca.chunksize[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3L compressing\n",
      "3L remapping\n",
      "3R compressing\n",
      "3R remapping\n",
      "2L compressing\n",
      "2L remapping\n",
      "2R compressing\n",
      "2R remapping\n",
      "X compressing\n",
      "X remapping\n"
     ]
    }
   ],
   "source": [
    "alt_list = []\n",
    "for cix, contig in enumerate(contigs):\n",
    "\n",
    "    sites_pos = allel.SortedIndex(called_sites[contig][\"variants/POS\"])\n",
    "    bial_pos, bial_alt, bial_ref = find_phase2_bialleleic_sites(contig)\n",
    "    loc = sites_pos.locate_keys(bial_pos)\n",
    "\n",
    "    alleles=da.hstack((bial_ref.reshape((-1, 1)), bial_alt))\n",
    "\n",
    "    # reduce to biallelic sites all samples still\n",
    "    print(contig, \"compressing\")\n",
    "    gt_a = allel.GenotypeDaskArray(calldata[contig][\"calldata/GT\"]).compress(loc)\n",
    "\n",
    "    print(contig, \"remapping\")\n",
    "    mapping = allel.create_allele_mapping(\n",
    "        ref=np.compress(loc, called_sites[contig][\"variants/REF\"]),\n",
    "        alt=np.compress(loc, called_sites[contig][\"variants/ALT\"]),\n",
    "        alleles=alleles)\n",
    "\n",
    "    count_alts = gt_a.map_alleles(mapping).to_n_alt(fill=-1)\n",
    "    \n",
    "    alt_list.append(count_alts)\n",
    "\n",
    "    # transpose and rechunk for scipy dist object\n",
    "    ca = count_alts.T\n",
    "    ratio = ca.shape[0] / ca.chunksize[0]\n",
    "    newchunks = (ca.shape[0], int(ca.chunksize[1] / ratio))\n",
    "    ca = ca.rechunk(chunks=newchunks)\n",
    "    nchunks = len(ca.chunks[1])\n",
    "\n",
    "    D = ca.map_blocks(\n",
    "        trans_d, \n",
    "        metric=cib_dist_nmissing,\n",
    "        chunks=((1,), tuple(np.repeat(1, nchunks))), \n",
    "        dtype=float, \n",
    "        drop_axis=(0, ), \n",
    "        new_axis=(0, ))\n",
    "    \n",
    "    X = ca.map_blocks(\n",
    "        trans_d, \n",
    "        metric=count_nmissing,\n",
    "        chunks=((1,), tuple(np.repeat(1, nchunks))), \n",
    "        dtype=float, \n",
    "        drop_axis=(0, ), \n",
    "        new_axis=(0, ))\n",
    "    \n",
    "    h[cix] = D.compute().sum(axis=1)\n",
    "    denom[cix] = X.compute().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    \"replicate-qc-{sset}\".format(sset=sampleset), \n",
    "    cityblock=h, \n",
    "    nsites=denom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
